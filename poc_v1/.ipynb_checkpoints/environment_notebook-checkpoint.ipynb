{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47230757",
   "metadata": {},
   "source": [
    "# AlphaGear POC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde7e2a7",
   "metadata": {},
   "source": [
    "\t- Game setting\n",
    "\t\t○ 1 agent only, 1 neutral\n",
    "\t- Environment\n",
    "\t\t○ State (one-hot encoded) = \n",
    "\t\t\t- [States owned] N \n",
    "\t\t\t- [States opponent owned] N\n",
    "\t\t\t- Soldiers on each square (max soldiers x N)^N\n",
    "\t\t\t- So if we play on even a toy map: 5 x 5 with max 10 soldiers, that is: 5 x 5 x (10 x 5) ^ 5 = 7E9 states\n",
    "\t\t○ State (functional approx)\n",
    "\t\t\t- State distribution [size M players x N tiles]\n",
    "\t\t○ Simple grid - POC_grid.xlsx\n",
    "\t\t\t- No fog\n",
    "\t\t○ Env_init\n",
    "\t\t\t- Randomly distribute the state w/ 3 troops\n",
    "\t\t○ Env_step\n",
    "\t\t\t- Dynamics\n",
    "\t\t\t\t□ Still dictated by the attacking logic \n",
    "\t\t\t\t\t® + Attacker gets 3 troops at the territory they attack from?\n",
    "\t\t\t\t\t® Attacker gets + 3 troops for each group of territories they own\n",
    "\t\t\t\t□ Need to move all troops over\n",
    "\t- Actions\n",
    "\t\t○ [Attack up, Attack down, Attack left, Attack right, Do nothing]\n",
    "\t\t\t- Attacking into your own territory effectively does nothing\n",
    "\t\t○ 1 per turn\n",
    "\t\t○ Exploration with softmax\n",
    "\t\t\n",
    "\t- Rewards\n",
    "\t\t○ +1 for winning the game\n",
    "\t\t\n",
    "    - Algorithm = Expected Sarsa![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04a0d1",
   "metadata": {},
   "source": [
    "## Creating the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "55b6b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Coursera_lab_files import environment\n",
    "from dynamics import *\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "class RiskEnvironmentPOC(environment.BaseEnvironment):\n",
    "    def __init__(self):\n",
    "        self.current_state = None\n",
    "        self.count = 0\n",
    "        \n",
    "    def env_init(self, env_info):\n",
    "        # users set this up\n",
    "        \"\"\"Setup for the environment called when the experiment first starts.\n",
    "\n",
    "        Note:\n",
    "            Initialize a tuple with the reward, first state observation, boolean\n",
    "            indicating if it's terminal.\n",
    "        \"\"\"\n",
    "        self.state = np.array([0, np.zeros(env_info['states']), False]) #reward, state init, boolean\n",
    "        # number of states - should be a perfect square                 \n",
    "        # Does not need to return anything\n",
    "    \n",
    "    def env_start(self):\n",
    "        \"\"\"The first method called when the experiment starts, called before the\n",
    "        agent starts.\n",
    "\n",
    "        Returns:\n",
    "            The first state observation from the environment.\n",
    "        \"\"\"\n",
    "        # Set up the board with troops\n",
    "        # where 'positive = agent troops' and 'negative = neutral troops'\n",
    "        states_remaining = list(range(len(self.state[1]))) # list of states to choose from\n",
    "        turn = True # whether it is the turn of the agent to be assigned troops somewhere or not\n",
    "        \n",
    "        for i in range(len(self.state[1])):\n",
    "            \n",
    "            # determine index to assign troops to\n",
    "            index = random.choice(states_remaining)\n",
    "            states_remaining.remove(index)\n",
    "            \n",
    "            if turn == True:\n",
    "                self.state[1][index] = 3\n",
    "                turn = False\n",
    "            else:\n",
    "                self.state[1][index] = -3\n",
    "                turn = True  \n",
    "\n",
    "        self.current_state = self.state[1]\n",
    "        return self.current_state\n",
    "    \n",
    "    def env_step(self, action):\n",
    "        \"\"\"A step taken by the environment.\n",
    "\n",
    "        Args:\n",
    "            action: The action taken by the agent\n",
    "\n",
    "        Returns:\n",
    "            (float, state, Boolean): a tuple of the reward, state observation,\n",
    "                and boolean indicating if it's terminal.\n",
    "        \"\"\"\n",
    "        #############\n",
    "        # Dynamics\n",
    "        ### functions that take in the action and return the observation\n",
    "        # X = func(action)\n",
    "        # initialize\n",
    "        terminal = False\n",
    "        reward = 0.0\n",
    "        observation = action_state(self.state[1], action, len(self.state)*5) # state from dynamics\n",
    "        #############\n",
    "        \n",
    "        # use the above observations to decide what the reward will be, and if the agent is in a terminal state.\n",
    "        \n",
    "        #############\n",
    "        # Reward function\n",
    "        \n",
    "        # In this simple implementation (where + numbers represent the agents territories, the game is over when all numbers in the grid are positive)\n",
    "        array_negative = np.where(observation < 0, observation, 0) # replaces all positive value cells with 0\n",
    "        comparison = np.where(observation < 0, observation, 0) == np.zeros(range(len(self.state))) # True if no negatives\n",
    "        \n",
    "        if comparison.all():\n",
    "            # agent wins                \n",
    "            terminal = True\n",
    "            reward += 1\n",
    "         \n",
    "        else: \n",
    "            # continue\n",
    "            terminal = False\n",
    "            reward += 0.0\n",
    "        #############\n",
    "        \n",
    "        self.state[0] = reward\n",
    "        self.state[1] = observation\n",
    "        self.state[2] = terminal\n",
    "        self.current_state = self.state[1]\n",
    "        \n",
    "        self.reward_obs_term = (reward, observation, terminal)\n",
    "        return self.reward_obs_term\n",
    "    \n",
    "    def env_cleanup(self):\n",
    "        return None\n",
    "    \n",
    "    def env_message(self):\n",
    "        return None\n",
    "    \n",
    "    def env_print_state(self):\n",
    "        dim = int(np.sqrt(len(self.state[1])))\n",
    "        grid = self.state[1].reshape((dim,dim), order = \"C\")\n",
    "\n",
    "        # determine who owns which territories\n",
    "        player_squares = np.zeros(len(self.state[1]))\n",
    "        for i in range(len(player_squares)):\n",
    "            if self.state[1][i] > 0:\n",
    "                player_squares[i] = 1\n",
    "            else:\n",
    "                player_squares[i] = 0\n",
    "        grid_player = player_squares.reshape((dim,dim), order = \"C\")\n",
    "        \n",
    "        cmap = ListedColormap(['blue', 'grey']) # player colors\n",
    "        ax = sns.heatmap(grid_player, cmap=cmap, annot=grid, cbar=False)\n",
    "        \n",
    "        #plot the borders\n",
    "        for i in range(grid.shape[1]+1):\n",
    "            if (i == int(dim/2+1)) or (i == int(dim/2)):  \n",
    "                ax.axvline(i, color='white', lw=2)\n",
    "                ax.axhline(i, color='white', lw=2)\n",
    "\n",
    "        plt.xticks(np.arange(0, dim+1, 1.0))\n",
    "        plt.yticks(np.arange(0, dim+1, 1.0))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6c0b0",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1cd8ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = RiskEnvironmentPOC()\n",
    "env_info = {'states':25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "858b60af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment.env_init(env_info)\n",
    "environment.env_start()\n",
    "# print(environment.state)\n",
    "environment.env_print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d2bb104",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-2f251069d2c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# action 0 should be from top right cell -> up (= nothing)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_print_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# action 1 should be from top right cell -> down\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-3a83bb22f117>\u001b[0m in \u001b[0;36menv_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# state from dynamics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[1;31m#############\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\alpha_gear\\poc_v1\\dynamics.py\u001b[0m in \u001b[0;36maction_state\u001b[1;34m(state, action_num, num_actions)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# each territory has 5 actions: up, down, left, right, do nothing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# if out of bounds, then it doesn't do anything\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0maction_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0maction_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# down\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# action 0 should be from top right cell -> up (= nothing)\n",
    "reward, obs, term = environment.env_step(0)\n",
    "environment.env_print_state()\n",
    "\n",
    "# action 1 should be from top right cell -> down\n",
    "reward, obs, term = environment.env_step(1)\n",
    "environment.env_print_state()\n",
    "\n",
    "# action 2 should be from top right cell -> left (= nothing)\n",
    "reward, obs, term = environment.env_step(2)\n",
    "environment.env_print_state()\n",
    "\n",
    "# action 3 should be from top right cell -> right \n",
    "reward, obs, term = environment.env_step(3)\n",
    "environment.env_print_state()\n",
    "\n",
    "# action 4 should be to do nothing\n",
    "reward, obs, term = environment.env_step(5)\n",
    "environment.env_print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58400c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a9119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
