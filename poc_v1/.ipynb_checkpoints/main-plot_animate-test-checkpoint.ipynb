{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb760a9d",
   "metadata": {},
   "source": [
    "# AlphaGear POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703e4fd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FuncAnimation\n\u001b[0;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcycler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorbar\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\colorbar.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook, collections, cm, colors, contour, ticker\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmartist\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpatches\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\contour.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MouseButton\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Text\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpath\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\backend_bases.py:45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     46\u001b[0m     _api, backend_tools \u001b[38;5;28;01mas\u001b[39;00m tools, cbook, colors, _docstring, text,\n\u001b[0;32m     47\u001b[0m     _tight_bbox, transforms, widgets, get_backend, is_interactive, rcParams)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pylab_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Gcf\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToolManager\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\text.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfont_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FontProperties\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FancyArrowPatch, FancyBboxPatch, Rectangle\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtextpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextPath, TextToPath  \u001b[38;5;66;03m# noqa # Logically located here\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     Affine2D, Bbox, BboxBase, BboxTransformTo, IdentityTransform, Transform)\n\u001b[0;32m     23\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1612\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "\n",
    "# Basic functions\n",
    "from rl_glue import RLGlue\n",
    "from environment import BaseEnvironment\n",
    "from agent import BaseAgent\n",
    "\n",
    "# Custom function for the environment and reward function\n",
    "from environment_alphagear import RiskEnvironmentPOC\n",
    "\n",
    "# Custom functions for the agent functions\n",
    "from agent_funcs import *\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d93a4e",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d94967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSarsaAgent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        self.name = \"expected_sarsa_agent\"\n",
    "        \n",
    "    def agent_init(self, agent_config):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the agent.\n",
    "\n",
    "        Assume agent_config dict contains:\n",
    "        {\n",
    "            network_config: dictionary,\n",
    "            optimizer_config: dictionary,\n",
    "            replay_buffer_size: integer,\n",
    "            minibatch_sz: integer, \n",
    "            num_replay_updates_per_step: float\n",
    "            discount_factor: float,\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], agent_config['minibatch_sz']) #, agent_config.get(\"seed\")) #in agent_funcs\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.optimizer = Adam(self.network.layer_sizes, agent_config[\"optimizer_config\"])\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "        self.step_by_step = agent_config['step_by_step']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState() #agent_config.get(\"seed\")\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "    # Work Required: No.\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        \n",
    "        The state is normalized by dividing by the potential resources of both players\n",
    "        Note: slightly different than the approach here: http://kth.diva-portal.org/smash/get/diva2:1514096/FULLTEXT01.pdf        \n",
    "        Hence the span is possibly, but not always, (-1, 1)\n",
    "        Returns:\n",
    "            the action. \n",
    "        \"\"\"\n",
    "        # abs_state = np.abs(state[0])\n",
    "        # abs_troops_sum = np.nansum(abs_state, dtype = np.float64)\n",
    "        # state[0] = state[0] / abs_troops_sum \n",
    "        # tau_corrected = self.tau / abs_troops_sum\n",
    "        \n",
    "        # REDUCE BASED ON NUMBER OF ACTIONS NOT TAKEN YET\n",
    "        \n",
    "        action_values = self.network.get_action_values(state)\n",
    "        probs_batch = softmax(action_values, self.tau)\n",
    "        action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        \n",
    "        if self.step_by_step:\n",
    "            print(\"The State is:\")\n",
    "            print(state)\n",
    "            print(\"The Action Values are:\")\n",
    "            print(action_values)\n",
    "            print(\"With Tau = \" + str(self.tau) + \"  the Softmax probabilities are:\")\n",
    "            print(probs_batch)\n",
    "            print(\"The selected action is: \" +str(action) + \" which should be equal to the next action picked below\")\n",
    "            \n",
    "        return action\n",
    "\n",
    "    # Work Required: No.\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state])\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "    # weights update using optimize_network, and updating last_state and last_action.\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, False, state)\n",
    "           \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network\n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau)\n",
    "                \n",
    "                \n",
    "        # Update the last state and last action.\n",
    "        self.last_state = state\n",
    "        self.last_action = action      \n",
    "        \n",
    "        return action\n",
    "\n",
    "    # update of the weights using optimize_network.\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state)\n",
    "\n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, True, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network\n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau)\n",
    "                    \n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6a35e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd77367",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Script is a combination of the experiment scripts from 2.3 and 4.2\n",
    "\n",
    "# Experiment parameters\n",
    "num_runs = 1 #100 The number of runs - take average to reduce variance\n",
    "num_episodes = 3000 # 2000 # The number of episodes in each run - it is a single game played to completion\n",
    "max_steps_episode = 400 #500\n",
    "step_by_step = False #plots information on a step by step basis\n",
    "animate = True #saves plots for further animation processing\n",
    "\n",
    "# Environment parameters\n",
    "num_states = 16 \n",
    "num_actions_per_state = 4\n",
    "num_actions = num_states*num_actions_per_state+1 \n",
    "num_hidden_units = (num_states*32)\n",
    "\n",
    "env_info = {'states':num_states, \"num_actions\":num_actions_per_state}\n",
    "\n",
    "environment = RiskEnvironmentPOC\n",
    "\n",
    "all_reward_sums = {} # Contains sum of rewards\n",
    "all_num_steps = {} \n",
    "all_state_visits = {} # Contains state visit counts during the last 10 episodes\n",
    "\n",
    "# Agent parameters\n",
    "agents = {\n",
    "    #\"Expected Sarsa\": ExpectedSarsaAgent\n",
    "    \"Expected Sarsa Agent\": ExpectedSarsaAgent\n",
    "    #,\"Expected Sarsa-tauB\": ExpectedSarsaAgent\n",
    "    #,\"Expected Sarsa-tauC\": ExpectedSarsaAgent\n",
    "    #,\"Expected Sarsa-tauD\": ExpectedSarsaAgent\n",
    "    #,\"Expected Sarsa-tauE\": ExpectedSarsaAgent\n",
    "}\n",
    "# should be 32 x states for the hidden variables\n",
    "agent_info = {\n",
    "    'network_config': {\n",
    "        'state_dim': num_states,\n",
    "        'num_hidden_units': num_hidden_units,\n",
    "        'num_actions': num_actions,\n",
    "        'num_hidden_layers': 2\n",
    "    },\n",
    "    'optimizer_config': {\n",
    "        'step_size': 1e-3,\n",
    "        'beta_m': 0.9, \n",
    "        'beta_v': 0.999,\n",
    "        'epsilon': 1e-8\n",
    "    },\n",
    "    'replay_buffer_size': 50000,\n",
    "    'minibatch_sz': 8,\n",
    "    'num_replay_updates_per_step': 4,\n",
    "    'gamma': 0.95,\n",
    "    'tau': 0.1,\n",
    "    'step_by_step': step_by_step\n",
    "}\n",
    "\n",
    "#tau = [0.1, 1, 10, 100]\n",
    "tau = [0.1]\n",
    "\n",
    "'''\n",
    "Tau observations\n",
    "0.01 is much too small initialy\n",
    "\n",
    "only tau = 100 showed the likelihood of finishing the map\n",
    "\n",
    "'''\n",
    "\n",
    "### EXPERIMENT SCRIPT\n",
    "\n",
    "count_agent = 0\n",
    "\n",
    "for algorithm in agents:\n",
    "    all_reward_sums[algorithm] = []\n",
    "    all_num_steps[algorithm] = []\n",
    "    all_state_visits[algorithm] = []\n",
    "    \n",
    "    if len(agents) > 1:\n",
    "        agent_info['tau'] = tau[count_agent]\n",
    "    \n",
    "    for run in tqdm(range(num_runs)):\n",
    "        # agent_info[\"seed\"] = run # This seeds the random number generator - not used\n",
    "        \n",
    "        # Agent initialization\n",
    "        rl_glue = RLGlue(environment, agents[algorithm])\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "        reward_sums = []\n",
    "        num_steps = []\n",
    "        #state_visits = np.zeros(num_states)\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            \n",
    "            # Animation config\n",
    "            if episode == (num_episodes-1) and animate == True: \n",
    "                # will save plots from the last episode\n",
    "                # https://stackoverflow.com/questions/6230353/how-to-create-gif-animation-from-a-stack-of-jpgs\n",
    "                rl_glue.animation()\n",
    "            \n",
    "            # Standard episodic run\n",
    "            rl_glue.rl_episode(max_steps_episode)\n",
    "            reward_sums.append(rl_glue.rl_return())  # rl_return() returns the total reward across episodes after the env initialization          \n",
    "            num_steps.append(rl_glue.rl_num_steps())\n",
    "            '''\n",
    "            if episode < num_episodes - 10:\n",
    "                # Runs an episode without recording... due to the variance?\n",
    "                rl_glue.rl_episode(max_steps_episode) \n",
    "            else: \n",
    "                # Runs an episode while keeping track of visited states ONLY FOR LAST 10 EPISODES\n",
    "                state, action = rl_glue.rl_start()\n",
    "                #state_visits[state] += 1\n",
    "                is_terminal = False\n",
    "                while not is_terminal:\n",
    "                    \n",
    "                    reward, state, action, is_terminal = rl_glue.rl_step()\n",
    "                    #state_visits[state] += 1\n",
    "                \n",
    "            reward_sums.append(rl_glue.rl_return())\n",
    "            '''    \n",
    "            \n",
    "        all_reward_sums[algorithm].append(reward_sums)\n",
    "        all_num_steps[algorithm].append(num_steps)\n",
    "        #all_state_visits[algorithm].append(state_visits)\n",
    "    \n",
    "    count_agent += 1\n",
    "\n",
    "# plot results\n",
    "# Clear plot\n",
    "plt.clf()\n",
    "    \n",
    "for algorithm in agents:\n",
    "    # Plots the AVERAGE sum of rewards across runs for an Episode \n",
    "    plt.plot(np.mean(all_reward_sums[algorithm], axis=0), label=algorithm)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\",rotation=0, labelpad=40)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dcc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "for algorithm in agents:\n",
    "    # Plots the AVERAGE sum of rewards across runs for an Episode \n",
    "    plt.plot(np.mean(all_num_steps[algorithm], axis=0), label=algorithm)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Number\\n of steps\\n during\\n episode\",rotation=0, labelpad=40)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_num_steps[algorithm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcdf38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_glue.env_print_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d9891",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Meta parameter tuning\n",
    "    - If softmax - Reduce tau as a function of the number of episodes?\n",
    "        - However, softmax is not best for a large action space\n",
    "        - Might a gaussian policy parameterization be better?\n",
    "    - Increase the tau / discount / learning rate factor as a function of the number of episodes?\n",
    "    - Apply heuristics to reward function\n",
    "        - (Done) -ve per step\n",
    "        - (Done) +ve per taking territory\n",
    "        - Maybe the reward scaling isn't good since the action values are randomly initialized between 0 and 1\n",
    "            - (Done) 100 for winning\n",
    "- Reduce action-value dimensions and/or improve the NN\n",
    "    - (Done) Add another layer to the NN\n",
    "    - (Done) Cap the number of troops on a territory before 'place' step\n",
    "    - Mask not possible actions\n",
    "\n",
    "- - Accomodate new environments each run (rather than just a single one)\n",
    "    - Move environment initialization into Run loop? Or for each new episode?\n",
    "- Plot the average visits on each square (see assignment 2.3)\n",
    "    - Note: states are actually high dimension - can't keep track of visits on a state, but maybe attacks to and from?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a306d6",
   "metadata": {},
   "source": [
    "##### Major Changes\n",
    "- Changed the NN weight initialization\n",
    "- Added another layer\n",
    "- Normalized the state before feeding NN\n",
    "- Added state troop number normalization\n",
    "- Applied heuristics to the reward function (see above)\n",
    "- Added plotting and animation functions to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1d120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
